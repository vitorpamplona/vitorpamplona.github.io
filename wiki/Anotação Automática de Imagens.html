<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">

<head>
  <meta http-equiv="content-type" content="text/html; charset=utf-8" />
  <meta http-equiv="content-language" content="pt-BR,en-US">
  <meta name="copyright" content="Vitor Fernando Pamplona">
  <meta name="robots" content="follow">
  <meta name="revisit-after" content="3 days">
  <meta name="rating" content="general">
  <meta property="fb:admins" content="1352207037" />
  <meta name="verify-v1" content="PEmbDgMrDta4EuJW0kdAcwnNetOEgCFwPEg+108t7/E=" />

  <meta name="description" content="   Algoritmos de anotação automática de imagens são formalmente descritos como: dado uma imagem  I  com características visuais  V    I   = { v   1 ,  v   2 ,...,  v    n  } e um conjunto de palavras-chave  W    I   = { w   1 ,  w   2 ,...,  w    m  }, encontre um subconjunto  W    p   ⊂  W    I  , que descreve apropriadamente a imagem  I . Historicamente, estas anotações eram informadas por bibliotecários para cada imagem separadamente, um processo exaustivo e caro nos tempos atuais [Lavrenko et al.2003]. , Author: Vitor Fernando Pamplona" />
  <meta name="keywords" content="Vitor, Vitor Pamplona, Vitor Fernando Pamplona, Rails, Ruby, Java, C++, Mestrado, Doutorado, Engenharia de Software, Usabilidade, Efetividade, Boo, Carreira" />
  <meta name="author" content="Vitor Fernando Pamplona" />
  <link rel="alternate" type="application/rss+xml" title="Vitor Pamplona's Feed" href="http://vitorpamplona.com/lastChangesRss.pr" />
  


  <title>Anotação Automática de Imagens :: Vitor Pamplona</title>

  <link rel="stylesheet" type="text/css" media="all" href="../interface/css/style.css" />

  <link rel="StyleSheet" href="../interface/includes/tree/dtree.css" type="text/css"> </link>
  <script type="text/javascript" src="../dtree.pr.html"></script>        


	<script class="javascript" src="http://vitorpamplona.com/deps/syntax/shCore.js"></script>
<script class="javascript" src="http://vitorpamplona.com/deps/syntax/shBrushJava.js"></script>
<script class="javascript" src="http://vitorpamplona.com/deps/syntax/shBrushRuby.js"></script>

<link href="http://vitorpamplona.com/deps/syntax/SyntaxHighlighter.css" type=text/css rel=stylesheet>

<script type="text/javascript">
var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
</script>
<script type="text/javascript">
var pageTracker = _gat._getTracker("UA-1248613-2");
pageTracker._initData();
pageTracker._trackPageview();

function f_unload() {pageTracker._trackPageview("/endpage");}
window.onunload = f_unload;
</script>
               
	<style>
#pageshare {
  position:fixed; 
  top:15%; 
  margin-left:-90px; 
  float:left; 
  border-radius:5px;-moz-border-radius:5px;-webkit-border-radius:5px;
  padding:0 0 2px 0;

}
#pageshare .sbutton {float:left;clear:both;margin:5px 5px 0 5px;}
.fb_share_count_top {width:48px !important;}
.fb_share_count_top, .fb_share_count_inner {-moz-border-radius:3px;-webkit-border-radius:3px;}
.FBConnectButton_Small, .FBConnectButton_RTL_Small {width:49px !important; -moz-border-radius:3px;/*bs-fsmsb*/-webkit-border-radius:3px;}
.FBConnectButton_Small .FBConnectButton_Text {padding:2px 2px 3px !important;-moz-border-radius:3px;-webkit-border-radius:3px;font-size:8px;}
</style>        

<!-- Place this tag in your head or just before your close body tag -->
<script type="text/javascript" src="https://apis.google.com/js/plusone.js"></script>



<script language="JavaScript">
	function toggleVisibility(me){
   		el = document.getElementById(me);
		if (el.style.display=='none'){
			el.style.display='';
		} else {
			el.style.display='none';
		}
	}	
</script>
  
</head>

<body>

    <div class="page-container">
	
   
    <!-- Sitename and Banner -->
	<div class="site-name">
		 Vitor Pamplona
	</div>	
	  	<div class="site-slogan">
  <p>Innovation on Vision: <a href="http://eyenetra.com">Imaging</a> , <a href="http://tailoreddisplays.com">Enhancement</a> and <a href="http://vitorpamplona.com/wiki/Photorealistic%20Models%20for%20Pupil%20Light%20Reflex%20and%20Iridal%20Pattern%20Deformation">Simulation</a></p>
		</div>
		
    	<div class="nav-main nav-main-font">			
		<!-- Main Navigation -->
		<ul>
                



      	<li><a href="index.html"  class="selected" >Highlights</a></li>

		<li><a href="Papers.html"  class="selected" >Research Projects</a></li>



<li><a href="../tagIndex.pr.html" >
Personal Thoughts</a></li>

		<li><a href="http://www.twitter.com/vitorpamplona" >Live Updates</a></li>

		<li><a href="https://plus.google.com/101404049137638062356/posts">Shared Stories</a></li>

        	
                


    	  <li><a href="Curriculum.html">Curriculum</a></li>

		        	
        			
		
						
		</ul>
		
	</div>
	
	<div class="buffer"></div>

	<!-- WRAP CONTENT AND SIDEBAR -->
    <div class="container-content-sidebar-front">

	<!-- div class="left-menu">
	    <div class="left-sidebarbox-border bg-blue02">
	    	<div class="sidebarbox-title-shading bg-blue05">
	    	     Menu
	    	</div>
	    	
	    	<ul>
        	<li><a href="/wiki/"  class="selected" >Front Page</a></li>
        	<li><a href="/lastChanges.pr"  class="selected" >Blog</a></li>
        	<li><a href="/new.pr"  class="selected" >New Page</a></li>
			        	<li><a href="/showSignUp.pr"  class="selected" >Sign Up</a></li>        	
        	        		    	</ul>
	    </div>		
	</div -->
	<!-- Continue in index.vm -->      
	
		

<div class="content content-font">
		
	
		<div class="contentbox-full">	

	
			<div class="left-menu">	
																					
			

			</div>
				
						<!-- SIDEBAR -->		
	  		<div class="sidebar sidebar-font">
	  			
	  		
									
								
												
				

			</div>

			<h1 class="line-black"> Anotação Automática de Imagens</h1> 

						        				


								<div class="contentbox-noshading">





					<p>   Algoritmos de anotação automática de imagens são formalmente descritos como: dado uma imagem <em> I </em> com características visuais <em> V </em> <sub> <em> I </em> </sub> = {<em> v </em> <sub> 1 </sub>, <em> v </em> <sub> 2 </sub>,..., <em> v </em> <sub> <em> n </em> </sub>} e um conjunto de palavras-chave <em> W </em> <sub> <em> I </em> </sub> = {<em> w </em> <sub> 1 </sub>, <em> w </em> <sub> 2 </sub>,..., <em> w </em> <sub> <em> m </em> </sub>}, encontre um subconjunto <em> W </em> <sub> <em> p </em> </sub> ⊂ <em> W </em> <sub> <em> I </em> </sub>, que descreve apropriadamente a imagem <em> I </em>. Historicamente, estas anotações eram informadas por bibliotecários para cada imagem separadamente, um processo exaustivo e caro nos tempos atuais [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Lavrenko2003">Lavrenko et al.2003</a>]. </p> <p> A área de Visão Computacional (VC) tem se esforçado em extrair o conjunto de características necessárias para representar uma imagem isolada. A similaridade de saliências entre várias imagens previamente anotadas permite identificar palavras-chave compartilhadas e, através da freqüência em que ocorrem, associar palavras-chaves diretamente às saliências. Apesar do avanço da área, as anotações manuais ainda são exigidas e os algoritmos dependem diretamente da qualidade destas anotações. Vale lembrar que o propósito dos algoritmos de visão computacional são diferentes dos de busca e anotação de imagens [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Smeulders2000">Smeulders et al.2000</a>]. Uma segmentação completa da imagem em objetos, como quer a VC, pode não melhorar a nomeação de palavras-chave relevantes. </p> <p> Motivadas pelos serviços de busca, técnicas de recuperação de informações (RI) são utilizadas para refinar o conjunto de palavras-chave previamente identificado pela VC, acrescentando um índice de relevância à palavra-chave a fim de organizar os resultados de uma busca. Recentemente, técnicas de processamento de linguagem natural (PLN) estão sendo utilizadas para remover a classificação manual, substituindo-a por uma compilação do texto associado a imagem. As imagens isoladas possuem apenas informação léxica e sintática (posição de objetos, cores, freqüência, etc), enquanto que imagens associadas a um texto possuem informação temporal, semântica ou contextual. [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Feng2008">Feng and Lapata2008</a>]. </p> <p> Neste artigo, descrevo o modelo probabilista de Feng e Lapata [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Feng2008">Feng and Lapata2008</a>] que, através de técnicas conhecidas de VC e PLN, cria um estimador para as palavras-chave mais relevantes de uma imagem. O modelo assume apenas que há uma descrição da imagem e que pode haver um documento associado a imagem. Tanto a descrição da imagem quanto o texto possuem muito ruído. A validação do modelo foi feita utilizando um conjunto de notícias extraídas do site BBC News. Os resultados indicam que o modelo a ser descrito é, em média, 50% superior em todos as comparações com o estado da arte. Você pode acompanhar o artigo pela apresentação abaixo. </p> <div style="text-align: center"> <object classid="clsid:D27CDB6E-AE6D-11cf-96B8-444553540000" codebase="http://download.macromedia.com/pub/shockwave/cabs/flash/swflash.cab#version=6,0,29,0" width="425" height="355"> <param name="movie" value="http://static.slideshare.net/swf/ssplayer2.swf?doc=imgtag-1227306887154605-9&stripped_title=anotao-de-imagens-presentation" /> <param name="quality" value="high" /> <param name="menu" value="false" /> <param name="wmode" value="" /> <embed src="http://static.slideshare.net/swf/ssplayer2.swf?doc=imgtag-1227306887154605-9&stripped_title=anotao-de-imagens-presentation" wmode="" quality="high" menu="false" pluginspage="http://www.macromedia.com/go/getflashplayer" type="application/x-shockwave-flash" width="425" height="355"> </embed> </object> </div> <h2 class="section"> <a name="htoc2" title="htoc2"></a> 2     Alguns Trabalhos Relacionados </h2> <! - - - - - - - - - - - - - - SEC END - - - - - - - - - - - - - - > <p> Até onde conhecemos, os trabalhos da área se classificam entre modelos de classificação, de co-ocorrência, de tradução, e de relevância. Vailaya et al. [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Vailaya2001">Vailaya et al.2001</a>] treinou classificadores Bayesianos para alguns contextos de alto nível a fim de categorizar as imagens em uma árvore semântica. As 6931 fotografias foram classificadas em 3 níveis: (i) em cenas internas e externas; (ii) as externas são subclassificadas em cidade e paisagem; as paisagens ainda podem ser (iii) pôr do sol, floresta ou montanha. O sistema alcançou 90,5% de acurácia no primeiro nível, 95% para o segundo e 96% para o terceiro. Smeulders [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Smeulders2000">Smeulders et al.2000</a>] fez um bom levantamento com mais de 200 referências sobre este tema. </p> <p> O trabalho de Mori el al. [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Mori1999">Mori et al.1999</a>] gera anotações baseado na co-ocorrência de palavras a partir de uma segmentação regular da imagem. Duygulu et al. [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Duygulu2002">Duygulu et al.2002</a>] continuou o trabalho utilizando o <em> normalized cuts </em> [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Shi1997">Shi and Malik1997</a>], um método de segmentação que modela uma imagem como um grafo unidirecional, onde uma aresta é formada entre cada par de pixels e é associado um peso proporcional a similaridade destes pixels. Arestas similares transformam-se em um segmento. Cada segmento recebe uma assinatura baseado em suas características e esta assinatura é procurada em todas as outras imagens do banco de dados. Através de métodos probabilistas o modelo relaciona automaticamente palavras-chave e assinaturas. </p> <p> Em uma linha diferente de trabalho, Jeon et al. [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Jeon2003">Jeon et al.2003</a>] faz uma analogia à associação de palavras-chave em regiões de uma imagem. Segundo ele, o problema é muito semelhante ao problema de busca de textos multi-língua. Seu modelo de relevância entre línguas (CMRM) utiliza técnicas de tradução automática para anotar as imagens e organizá-las por relevância. </p> <p> Lavrenko et al. [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Lavrenko2003">Lavrenko et al.2003</a>] divide uma imagem em regiões e encontra um conjunto de características relevantes para cada região. Se difere dos anteriores por possuir uma parte contínua, por não fazer assunções sobre a estrutura topológica e por possibilitar nível de detalhe no processamento das regiões. Feng et al. [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Feng2004">Feng et al.2004</a>] continuou o trabalho aplicando uma segmentação regular sobre as imagens e considerando também a posição relativa entre as regiões. A segmentação regular, além de aumentar a performance do método, simplificou o modelo pois o número de segmentos é sempre o mesmo. </p> <p> O modelo de Lavrenko et al. e sua continuação por Feng et al. [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Feng2004">Feng et al.2004</a>] é muito similar ao demonstrado neste artigo. O que os difere é a inclusão de uma probabilidade para anotar palavras-chave que não estão na descrição da imagem, mas sim em um documento relacionado a ela. </p> <h2 class="section"> <a name="htoc3" title="htoc3"></a> 3     Modelo de Feng e Lapata </h2> <! - - - - - - - - - - - - - - SEC END - - - - - - - - - - - - - - > <p> O método desenvolvido por Feng e Lapata [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Feng2008">Feng and Lapata2008</a>] é uma extensão ao modelo de anotações de relevância contínua de Lavrenko [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Lavrenko2003">Lavrenko et al.2003</a>], onde as imagens isoladas anotadas manualmente foram substituídas por imagens em páginas de notícias que estão disponíveis livremente e em grande quantidade na internet. Os <em> captions </em> HTML das imagens sugerem as palavras-chave iniciais, dispensando a etapa de anotar imagens manualmente para treinamento do sistema. Feng e Lapata utilizam algoritmos de VC e PLN para melhorar e reorganizar estas palavras-chave. </p> <p> Para ser válido, o modelo faz algumas assunções: </p> <ul class="itemize"> <li class="li-itemize"> O <em> captions </em> HTML da imagem devem descrever diretamente ou indiretamente a imagem. Os <em> captions </em> podem ser denotativos, descrevendo os objetos na imagem, ou conotativos, indicando atitudes e ações apresentadas na imagem. Ambas são palavras-chave consistentes, mas nem todas as palavras dos <em> captions </em> são palavras-chave relevantes. Uma análise inicial identificou que os <em> captions </em> do banco utilizado (BBC News) descrevem o conteúdo da imagem em 90% das vezes. </li> <li class="li-itemize"> Pode não ser possível nomear todos os objetos na imagem, mas os objetos mais relevantes devem constar na classificação. Como é um algoritmo de recuperação de informações e não de visão computacional, esta assunção não chega a causar problemas. </li> <li class="li-itemize"> O documento associado a imagem descreve de alguma forma a imagem. Para artigos de notícia, isto geralmente é verdade. </li> </ul> <! - - - - - - - - - - - - - - TOC subsection Descrição do Modelo - - - - - - - - - - - - - - > <h3 class="subsection"> <! - - - - - - - - - - - - - - SEC ANCHOR - - - - - - - - - - - - - - > <a name="htoc4" title="htoc4"></a> 3.1     Descrição do Modelo </h3> <p> O modelo estima para uma imagem não conhecida <em> I </em>, a probabilidade das palavras-chave <em> W </em> <sub> <em> I </em> </sub> e das regiões <em> V </em> <sub> <em> I </em> </sub> estarem relacionadas através da equação:   </p> <div style="text-align: center"> <img src="http://vitorpamplona.com/deps/imgTag/img7.png" border="0" alt="Feng e Lapata" width="285" height="49" /> </div> onde <em> D </em> é o número de tuplas (imagem, palavra) no banco de treinamento, <em> V </em> <sub> <em> I </em> </sub> são as características visuais que representam <em> I </em>, <em> W </em> <sub> <em> I </em> </sub> são as palavras-chave de <em> I </em>, <em> s </em> é uma tupla (imagem, palavra) e <em> P </em> (<em> s </em>) é a probabilidade de <em> s </em> definido como uma distribuição uniforme <div style="text-align: center"> <img src="http://vitorpamplona.com/deps/imgTag/img11.png" border="0" alt=" " width="86" height="44" /> </div> <p> onde <em> N </em> <sub> <em> D </em> </sub> é o número de tuplas no banco. Para estimar a probabilidade das regiões da imagem <em> V </em> <sub> <em> I </em> </sub> ocorrerem dado <em> s </em> tem-se um produtório de cada região <em> v </em> <sub> <em> r </em> </sub> da imagem <em> I </em> ocorrer dado que <em> s </em> ocorra. </p> <div style="text-align: center"> <img src="http://vitorpamplona.com/deps/imgTag/img14.png" border="0" alt=" " width="168" height="62" /> </div> <p> onde <em> N </em> <sub> <em> V </em> <sub> <em> I </em> </sub> </sub> é o número de regiões na imagem <em> I </em>. Lavrenko et al. [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Lavrenko2003">Lavrenko et al.2003</a>] assume uma distribuição gaussiana para as regiões: </p> <div style="text-align: center"> <img src="http://vitorpamplona.com/deps/imgTag/img16.png" border="0" alt=" " width="350" height="63" /> </div> <p> onde <em> n </em> <sub> <em> s </em> <sub> <em> v </em> </sub> </sub> o número de regiões na imagem <em> s </em>, <em> v </em> <sub> <em> i </em> </sub> a assinatura para a região <em> i </em> em <em> s </em>, <em> k </em> a dimensão da assinatura e | ∑ | a matriz de covariância. Para simplificar, | ∑ | é assumida uma matriz diagonal (não há covariância) | ∑ | = β <em> M </em>, onde <em> M </em> é a matriz de identidade e β é um valor escalar otimizado para o conjunto de imagens. </p> <p> Ao contrário de Lavrenko et al., Feng e Lapata estimaram a probabilidade das palavras <em> W </em> ocorrerem dado <em> P </em> (<em> W </em> <sub> <em> I </em> </sub> | <em> s </em>) através de uma binomial múltipla </p> <div style="text-align: center"> <img src="http://vitorpamplona.com/deps/imgTag/img27.png" border="0" alt=" " width="308" height="50" /> </div> <p> onde <em> P </em> (<em> w </em> | <em> s </em>) denota a probabilidade do <em> w </em> - ésimo componente da ocorrer dado <em> s </em>. Para estimá-lo pode-se incluir o documento como </p> <div style="text-align: center"> <img src="http://vitorpamplona.com/deps/imgTag/img30.png" border="0" alt=" " width="331" height="31" /> </div> <p> onde α é um parâmetro de suavização configurado durante o desenvolvimento, <em> s </em> <sub> <em> a </em> </sub> é uma anotação de <em> s </em> e <em> s </em> <sub> <em> d </em> </sub> é o documento correspondente. Esta equação suaviza a influência nas palavras-chave anotadas e permite corrigir o efeito negativo do ruído no banco de imagens. Como as imagens estão implicitamente anotadas com a tag <em> caption </em> do HTML, não há garantias que todas as palavras são apropriadas. Ao considerar <em> P </em> <sub> <em> est </em> </sub> (<em> w </em> | <em> s </em> <sub> <em> d </em> </sub>) é possível anotar uma imagem com uma palavra que aparece no documento mas não está incluída no texto. </p> <p> Usando probabilidade frequentista, pode-se estimar <em> P </em> <sub> <em> est </em> </sub> (<em> w </em> | <em> s </em> <sub> <em> a </em> </sub>) </p> <div style="text-align: center"> <img src="http://vitorpamplona.com/deps/imgTag/img36.png" border="0" alt=" " width="194" height="46" /> </div> <p> onde µ é uma constante de suavização, <em> b </em> <sub> <em> w </em>, <em> s </em> <sub> <em> a </em> </sub> </sub> é 1 se <em> w </em> aparece em <em> s </em> <sub> <em> a </em> </sub> e 0 se <em> w </em> não aparece em <em> s </em> <sub> <em> a </em> </sub> e <em> N </em> <sub> <em> w </em> </sub> é um número de imagens que possuem <em> w </em> em sua anotação. O mesmo é feito com <em> P </em> <sub> <em> est </em> </sub> (<em> w </em> | <em> s </em> <sub> <em> d </em> </sub>) </p> <div style="text-align: center"> <img src="http://vitorpamplona.com/deps/imgTag/img42.png" border="0" alt=" " width="144" height="47" /> </div> <p> onde <em> N </em> <sub> <em> w </em>, <em> s </em> <sub> <em> d </em> </sub> </sub> é o número de vezes que <em> w </em> ocorre em <em> s </em> <sub> <em> d </em> </sub> e <em> N </em> <sub> <em> s </em> <sub> <em> d </em> </sub> </sub> é o número total de palavras no documento. <! - - - - - - - - - - - - - - TOC subsection Implementação - - - - - - - - - - - - - - > </p> <h3 class="subsection"> <! - - - - - - - - - - - - - - SEC ANCHOR - - - - - - - - - - - - - - > <a name="htoc5" title="htoc5"></a> 3.2     Implementação </h3> <! - - - - - - - - - - - - - - SEC END - - - - - - - - - - - - - - > <p> O modelo foi treinado com 2881 documentos com imagem retirados do site da BBC News. Primeiramente, um <em> part of speech tagger </em> classificou as palavras e um lemmatizer as normalizou. Os autores removeram todas as palavras exceto substantivos, verbos e adjetivos. O vocabulário total ficou em 8309 palavras. </p> <p> Para segmentar as imagens, os autores utilizaram um grid regular 6 x 5 a fim de evitar erros dos algoritmos de segmentação, simplificar a implementação e a estimativa de parâmetros. 46 características foram utilizadas para descrever cada região: média e desvio padrão dos componentes RGB, LUV, LAB; saída de uma transformacão DCT, saída de um <em> Gabor filtering </em>; saída de um algoritmo de deteccão de borda, e a divisão entre o número de pixels de borda e não borda de cada região. </p> <! - - - - - - - - - - - - - - TOC subsection Resultados - - - - - - - - - - - - - - > <h3 class="subsection"> <! - - - - - - - - - - - - - - SEC ANCHOR - - - - - - - - - - - - - - > <a name="htoc6" title="htoc6"></a> 3.3     Resultados </h3> <! - - - - - - - - - - - - - - SEC END - - - - - - - - - - - - - - > <p> O modelo desenvolvido tem uma precisão - número de anotações corretas pelo número total - de 14% se forem consideradas apenas as 10 palavras-chave com maior probabilidade e 9.72% com as melhores 20 palavras-chave. Avaliando o número de palavras-chave identificadas corretamente pelo número de palavras-chave identificadas manualmente para a mesma imagem, o modelo tem 27.95% com as melhores 10 palavras-chave e 36.77%. Em termos de comparacao, este modelo tem um ganho de 50% comparado com o método de Lavrenko et al [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Lavrenko2003">Lavrenko et al.2003</a>] em precisão e <em> recall </em>, independente do número de palavras-chave utilizadas para a anotação. </p> <! - - - - - - - - - - - - - - TOC section Análise Crítica - - - - - - - - - - - - - - > <h2 class="section"> <! - - - - - - - - - - - - - - SEC ANCHOR - - - - - - - - - - - - - - > <a name="htoc7" title="htoc7"></a> 4     Análise Crítica </h2> <! - - - - - - - - - - - - - - SEC END - - - - - - - - - - - - - - > <p> Ainda há um amplo campo de pesquisa em classificação e anotação de imagens quando estas possuem um texto associado. O ruído presente no vocabulário é muito grande e as técnicas ainda se baseiam em palavras-chave isoladas, sem contexto semântico, temporal ou estrutura hierárquica. A informação contida no texto é aproveitada somente em termos sintáticos. Palavras que não são substantivos, verbos e advérbios são sumariamente removidas do modelo. Se assumirmos que texto de busca possui informação semântica, os algoritmos atuais não poderiam utilizar esta informação. </p> <p> A etapa de segmentação e os algoritmos de identificação de características nas imagens, em todos os artigos lidos, podem ser melhorados. Tanto o grid regular quanto o <em> normalized cuts </em> tem suas limitações e a ambiguidade é grande. Algoritmos como o SIFT [<a href="Anotação&#32;Automática&#32;de&#32;Imagens.html#Lowe2004">Lowe2004</a>], por exemplo, podem identificar características em imagem de uma maneira que o mesmo ponto característico pode ser escalado ou rotacionado que permanecerá com a mesma assinatura. Apesar de apresentar problemas com texturas, o método é muito mais preciso que as informações de cor utilizadas neste artigo. </p> <p> Os modelos aqui descritos poderiam obter melhores resultados com o uso da wordnet para trabalhar com sinônimos, super-classes e especializações, gerando ao invés de palavras-chave isoladas, uma hierarquia com índice de relevância. Os algoritmos de categorização de imagens da VC também poderiam se beneficiar da wordnet, mesmo processando imagens isoladas. </p> <p> Nenhum artigo relatou o uso de tradutores automáticos para criar palavras-chave em outras línguas e facilitar a pesquisa multi-língua. Na internet é possível que a mesma imagem seja utilizada em várias páginas diferentes. Seria interessante, então, verificar se há melhoria na anotação automática usando várias fontes para o texto e para o <em> caption </em> das palavras. </p> <! - - - - - - - - - - - - - - TOC section References - - - - - - - - - - - - - - > <h2 class="section"> <! - - - - - - - - - - - - - - SEC ANCHOR - - - - - - - - - - - - - - > Referências </h2> <! - - - - - - - - - - - - - - SEC END - - - - - - - - - - - - - - > <dl class="thebibliography"> <dt class="dt-thebibliography"> <a name="Duygulu2002" title="Duygulu2002"></a> <font color="purple"> [</font> <font color="purple"> Duygulu </font> <font color="purple"> et al. </font> <font color="purple"> 2002] </font> </dt> <dd class="dd-thebibliography"> P. Duygulu, K. Barnard, J. de Freitas, and D. Forsyth. 2002. Object recognition as machine translation: learning a lexicon for a fixed image vocabulary. In <em> Proceedings of the 7th European Conference on Computer Vision </em>, pages 97 – 112, Copenhagen, Danemark. </dd> <dt class="dt-thebibliography"> <a name="Feng2008" title="Feng2008"></a> <font color="purple"> [</font> <font color="purple"> Feng and Lapata </font> <font color="purple"> 2008] </font> </dt> <dd class="dd-thebibliography"> Y. Feng and M. Lapata. 2008. Automatic image annotation using auxiliary text information. In <em> Proceedings of ACL-08 HLT </em>, pages 272 – 280. </dd> <dt class="dt-thebibliography"> <a name="Feng2004" title="Feng2004"></a> <font color="purple"> [</font> <font color="purple"> Feng </font> <font color="purple"> et al. </font> <font color="purple"> 2004] </font> </dt> <dd class="dd-thebibliography"> S. Feng, V. Lavrenko, and R. Manmatha. 2004. Multiple bernoulli relevance models for image and video annotation. In <em> Proceedings of the International Conference on Computer Vision and Pattern Recognition </em>, pages 1002 – 1009, Washington, DC. </dd> <dt class="dt-thebibliography"> <a name="Jeon2003" title="Jeon2003"></a> <font color="purple"> [</font> <font color="purple"> Jeon </font> <font color="purple"> et al. </font> <font color="purple"> 2003] </font> </dt> <dd class="dd-thebibliography"> J. Jeon, V. Lavrenko, and R. Manmatha. 2003. Automatic image annotation and retrieval using cross-media relevance models. In <em> In Proceedings of the 26th Intl. ACM SIGIR Conf. </em> Pages 119 – 126. </dd> <dt class="dt-thebibliography"> <a name="Lavrenko2003" title="Lavrenko2003"></a> <font color="purple"> [</font> <font color="purple"> Lavrenko </font> <font color="purple"> et al. </font> <font color="purple"> 2003] </font> </dt> <dd class="dd-thebibliography"> V. Lavrenko, R. Manmatha, and J. Jeon. 2003. A model for learning the semantics of pictures. In <em> Proceedings of the 16th Conference on Advances in Neural Information Processing Systems </em>, Vancouver, BC. </dd> <dt class="dt-thebibliography"> <a name="Lowe2004" title="Lowe2004"></a> <font color="purple"> [</font> <font color="purple"> Lowe </font> <font color="purple"> 2004] </font> </dt> <dd class="dd-thebibliography"> D. G. Lowe. 2004. Distinctive image features from scale-invariant keypoints. <em> International Journal of Computer Vision </em>, 60:91 – 110. </dd> <dt class="dt-thebibliography"> <a name="Mori1999" title="Mori1999"></a> <font color="purple"> [</font> <font color="purple"> Mori </font> <font color="purple"> et al. </font> <font color="purple"> 1999] </font> </dt> <dd class="dd-thebibliography"> Y. Mori, H. Takahashi, and R. Oka. 1999. Image-to-word transformation based on dividing and vector quantizing images with words. In <em> Proceedings of the 1st International Workshop on Multimedia Intelligent Storage and Retrieval Management </em>, Orlando, FL. </dd> <dt class="dt-thebibliography"> <a name="Shi1997" title="Shi1997"></a> <font color="purple"> [</font> <font color="purple"> Shi and Malik </font> <font color="purple"> 1997] </font> </dt> <dd class="dd-thebibliography"> J. Shi and J.   Malik. 1997. Normalized cuts and image segmentation. <em> IEEE Conference on Computer Vision and Pattern Recognition </em>, pages 731 – 737. </dd> <dt class="dt-thebibliography"> <a name="Smeulders2000" title="Smeulders2000"></a> <font color="purple"> [</font> <font color="purple"> Smeulders </font> <font color="purple"> et al. </font> <font color="purple"> 2000] </font> </dt> <dd class="dd-thebibliography"> A. W. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain. 2000. Content-based image retrieval at the end of the early years. <em> IEEE Transactions on Pattern Analysis and Machine Intelligence </em>, 22 (12): 1349 – 1380. </dd> <dt class="dt-thebibliography"> <a name="Vailaya2001" title="Vailaya2001"></a> <font color="purple"> [</font> <font color="purple"> Vailaya </font> <font color="purple"> et al. </font> <font color="purple"> 2001] </font> </dt> <dd class="dd-thebibliography"> A. Vailaya, M. Figueiredo, A. Jain, and H. Zhang. 2001. Image classification for content-based indexing. <em> IEEE Transactions on Image Processing </em>, 10:117 – 130. </dd> </dl>


					
<script type="text/javascript"><!--
google_ad_client = "pub-7740443081675700";
/* 728x90, criado 06/04/10 */
google_ad_slot = "4989522421";
google_ad_width = 728;
google_ad_height = 90;
//-->
</script>
<div id="ads"><script type="text/javascript" src="http://pagead2.googlesyndication.com/pagead/show_ads.js"> </script></div>


                                <div>
                                        <p  class="postedBy">Posted in Dec 18, 2008 by <a href="../userHistory.pr%3FpostUser=vfpamp.html">Vitor Pamplona</a> -

                                                                                                                                                <a href="../edit.pr%3Fkeyword=Anota%25C3%25A7%25C3%25A3o%2520Autom%25C3%25A1tica%2520de%2520Imagens.html">Edit</a> -
                                                                                        
                                        <a href="../history.pr%3Fkeyword=Anota%25C3%25A7%25C3%25A3o%2520Autom%25C3%25A1tica%2520de%2520Imagens.html">History</a>
                                        					</p>
                                </div>
				</div>
				
				

					</div>
			
		
	<div id="fb-root"></div>
<script>
(function(d, s, id) {
  var js, fjs = d.getElementsByTagName(s)[0];
  if (d.getElementById(id)) {return;}
  js = d.createElement(s); js.id = id;
  js.src = "//connect.facebook.net/en_US/all.js#xfbml=1";
  fjs.parentNode.insertBefore(js, fjs);
}(document, 'script', 'facebook-jssdk'));
</script> 

<script type="text/javascript">
String.prototype.endsWith = function(suffix) {
    return this.indexOf(suffix, this.length - suffix.length) !== -1;
};

var url = window.location.href;
if (url.indexOf("Front%20Page") < 0  && url.indexOf("wiki/;jsessionid") < 0 && url.indexOf("lastChanges") < 0 && !url.endsWith("/wiki/")) {
   document.write('<div class="fb-comments" style="margin-left: 15px; border-bottom: solid 1px #546F90;" data-href="' + url + '" data-num-posts="10" data-width="800"></div>');
}
</script>             

	
					<div class="contentbox-full">
       		<h1 class="line-black">Showing Comments</h1> 
			
				<div class="contentbox-full-border">
			<div class="content-padding">
				<div class="contentbox-noshading">
					<p>Muito interessante seu blog. Parabéns. <BR> <BR> <a href="http://demoniodemaxwell.wordpress.com" target="_blank">http://demoniodemaxwell.wordpress.com</a> / <BR> <BR> - - Demônio de Maxwell</p>
					<p class="postedBy">
					- - Posted in Dec 20, 2008 by <a href="../userHistory.pr%3FpostUser=164.41.201.81.html">164.41.201.81</a> 
										</p>
				</div>
			</div>	
		</div>	
		
				
</div>				

<div class="contentbox-full">
		<div id="divComment2"  style="display: none;">
		<a href="javascript:toggleVisibility('divComment');toggleVisibility('divComment2');">Add New Comment</a>
	</div>
	<h1>Add New Comment</h1>
	<div id="divComment">
		<form id="edit" name="edit" method="post" action="http://www.vitorpamplona.com/postComment.pr">
			<input type="hidden" name="keyword" value="Anotação Automática de Imagens" />
			<p><textarea style="width:96%" name="commentOnlyText" id="commentOnlyText" rows=10></textarea></p>
							<p>Your Name: <input type="text" name="postUserName" value="" /></p>
						<p>
<img src="http://www.vitorpamplona.com/flood.jpg" /><br>
Write the code showed above on the text below.
<br>
<input class="inputText" type="text" id="floodValidation" name="floodValidation" value="" size=20/>
</p>
<p></p>			<p><input type="submit" name="Save" value="Save"></p>	
			<br>
		</form>	
	</div>
</div>	
				
	<script class='javascript'>  
 //<![CDATA[  
if (! window.ActiveXObject ) {
     function FindTagsByName(container, Tag)  
     {  
         var elements = document.getElementsByTagName(Tag);  
         for (var i = 0; i < elements.length; i++)  
         { 
		 if (elements[i].innerHTML.indexOf("def ") > -1
		 ||  elements[i].innerHTML.indexOf("ActiveR") > -1
		 ||  elements[i].innerHTML.indexOf("find_by") > -1	
                 ||  elements[i].innerHTML.indexOf(" => ") > -1
		 ) 
		    elements[i].setAttribute("class", "java");
                 else
		    elements[i].setAttribute("class","java");
		 elements[i].setAttribute("name", "code");
		 elements[i].setAttribute("id", "code");
                 container.push(elements[i]);  
         }  
     }  
     var elements = [];  
     FindTagsByName(elements, "pre");  

  for(var i=0; i < elements.length; i++) {  
   if(elements[i].nodeName.toUpperCase() == "PRE") {  
    brs = elements[i].getElementsByTagName("br");  
    for(var j = 0, brLength = brs.length; j < brLength; j++) {  
       var newNode = document.createTextNode("\n");  
       elements[i].replaceChild(newNode,brs[0]);  
    }  
   }  
  }  

}

   dp.SyntaxHighlighter.HighlightAll("code", false, false);  
 //]]>  
</script>     
	 <div id='pageshare' title="">
	<div class='sbutton' id='fb'>
        <script type="text/javascript">
           var url = window.location.href;
           document.write('<' + 'iframe src="http://www.facebook.com/plugins/like.php?href=' + url + '&send=false&layout=box_count&width=58&show_faces=true&action=like&colorscheme=light&font&height=65" scrolling="no" frameborder="0" style="border:none; overflow:hidden; width:58px; height:65px;" allowTransparency="true"></iframe>');
        </script>
	</div>
	
       <div class='sbutton' id='rt'>

       <script type="text/javascript">
           var url = window.location.href;
           document.write('<' + 'script type="text/javascript" src="http://button.topsy.com/widget/retweet-big?nick=vitorpamplona&url=' + url + '"></' + 'script>');
       </script>
		
	</div>

<div class='sbutton' id='gp'>
   <!-- Place this tag where you want the +1 button to render -->
   <g:plusone size="tall"></g:plusone>
<div>

	<div class='sbutton' id='gb'>
		<a class='google-buzz-button' data-button-style='normal-count' href='http://www.google.com/buzz/post' title='post on google buzz'>
			<script src='http://www.google.com/buzz/api/button.js' type='text/javascript'></script>
		</a>
	</div>


</div>                     

	</div>
		
    <!-- END WRAP CONTENT AND SIDEBAR -->
	</div>		

    <!-- FOOTER -->
    <div class="postedBy"></div>

    <div class="footer footer-font">
       <p><b>Copyright ©Vitor Pamplona | All Rights Reserved</b>  
       <br>Powered by <a href="http://wiki.com.br">Priki</a>. Visite os projetos: <a href="http://eyenetra.com">EyeNetra.com</a> e <a href="http://goblink.co">goBlink.co</a>.   

				- <a href="../showLogin.pr.html" class="selected">Login</a>
        	        	
	</p>
    </div>
  </div>
</body>
</html>
